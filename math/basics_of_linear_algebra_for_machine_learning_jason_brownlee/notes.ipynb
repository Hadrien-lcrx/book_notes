{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e091d0c-3033-4106-9845-2ff92fd38929",
   "metadata": {},
   "source": [
    "# Basics of linear algebra for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b028fdb-fa30-44d4-b36e-59dff81621ac",
   "metadata": {},
   "source": [
    "## 01 - Introduction to linear algebra\n",
    "### Linear algebra\n",
    "Linear algebra is about linear combinations: using arithmetic on columns of numbers (vectors) and arrays of numbers (matrices) to create new columns and arrays of numbers. It's been formalized in the 1800s to find unknowns in systems of linear equations. \n",
    "\n",
    "A linear equation is a series of terms and mathematical operations where some terms are unknown, for example:  \n",
    "$y = 4 x + 1$\n",
    "\n",
    "They are called linear equations because they describe a line on a two-dimensional graph. We can line up a system of equations with two or more unknowns:  \n",
    "- $y = 0.1 x_{1} + 0.4 x_{2}$  \n",
    "- $y = 0.3 x_{1} + 0.9 x_{2}$  \n",
    "- $y = 0.2 x_{1} + 0.3 x_{2}$\n",
    "\n",
    "where\n",
    "- the column of $y$ values is a column vector of outputs from the equation\n",
    "- the two columns of float values are the data columns $a_{1}$ and $a_{2}$ forming the matrix $A$\n",
    "- the two unknown values $x_{1}$ and $x_{2}$ are the coefficients of the equation and form a vector of unknowns $b$ to be solved\n",
    "\n",
    "summarized in linear algebra as  \n",
    "$y = A \\cdot b$\n",
    "\n",
    "Such problems are challenging to solve because:\n",
    "- there are usually more unknowns than there are equations to solve\n",
    "- no single line can satisfy all of the equations without error\n",
    "\n",
    "Interesting problems are often described by system with an infinite number of solutions. This is the core of linear algebra as it relates to machine learning. The rest of the operations are about making such problems easier to understand and solve.\n",
    "\n",
    "### Numerical linear algebra\n",
    "Implementations of vector and matrix operations were initially implemented in FORTRAN with libraries such as:\n",
    "- LAPACK\n",
    "- BLAS\n",
    "- ATLAS\n",
    "\n",
    "Popular packages used nowadays in Python for example build on top of these libraries.\n",
    "\n",
    "### Linear algebra and statistics\n",
    "- using vector and matrix notation (multivariate statistics)\n",
    "- solving least squares and weighted least squares (linear regression)\n",
    "- estimating means and variance of data matrices\n",
    "- using the covariance matrix (multinomial Gaussian distributions)\n",
    "- leveraging the concepts above for data reduction with principal component analysis\n",
    "\n",
    "### Applications of linear algebra\n",
    "- matrices in engineering (line of springs)\n",
    "- graphs and networks (graph analysis)\n",
    "- Markov matrices, population, economics (population growth)\n",
    "- linear programming (simplex optimization method)\n",
    "- Fourier series - linear algebra for functions (signal processing)\n",
    "- linear algebra for statistics and probabilities (least squares for regression)\n",
    "- computer graphics (translation, rescaling, rotation of images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be1b683-0fa7-47bf-8f81-242afe2a145b",
   "metadata": {},
   "source": [
    "## 02 - Linear algebra and machine learning\n",
    "Linear algebra is the mathematics of data. Often recommended as a prerequisite to machine learning, it can make more sense to first build context of the applied machine learning process.\n",
    "\n",
    "### Reasons not to learn linear algebra\n",
    "- it's not required in order to use machine learning as a tool to solve problems\n",
    "- it's slow and might delay you achieving your goals\n",
    "- it's a huge field and not all of it is relevant to machine learning\n",
    "\n",
    "A breadth-first (results-first) approach can help build a skeleton and some context on which to build to deepen knowledge about how algorithms work or the math that underlies them.\n",
    "\n",
    "### Linear algebra notation\n",
    "You need to know how to read and write vector and matrix notation. It enables you to:\n",
    "- describe operations on data precisely\n",
    "- read descriptions of algorithms in textbooks\n",
    "- implement machine learning algorithms faster and more efficiently\n",
    "- interpret and implement new methods in research papers\n",
    "- describe your own methods to other practitioners\n",
    "\n",
    "### Linear algebra arithmetic\n",
    "You need to know how to perform arithmetic operations: add, subtract and multiply scalars, vectors and matrices. Matrix multiplication and tensor multiplication are often non-intuitive at first. Understanding vector and matrix operations is required to effectively read and write matrix notation.\n",
    "\n",
    "### Learn linear algebra for statistics\n",
    "Linear algebra is heavily used in multivariate statistics. To read and interpret statistics, you need to know the notation and operations of linear algebra, such as vectors used for means and variance, or covariance matrices describing the relationships between multiple Gaussian variables. Principal component analysis also leverages such methods.\n",
    "\n",
    "### Learn matrix factorization\n",
    "Matrix factorization, is also called matrix decomposition. You need to know how to factorize a matrix and what it means. Matrix factorization is necessary for more complex operations in linear algebra (matrix inverse) and machine learning (least squares). Different matrix factorization exist, such as singular-value decomposition. To read and interpret higher-order matrix operations, matrix factorization is required.\n",
    "\n",
    "### Learn linear least squares\n",
    "Matrix factorization can be used to solve linear least squares. Problems where there is no line able to fit the data without error can be solved using the least squares method, called linear least squares in linear algebra. Linear least squares are used in regression models, and in a range of machine learning algorithms.\n",
    "\n",
    "### One more reason\n",
    "Seeing how the operations work on real data will help you develop a strong intuition for the methods. You will experience knowledge buzz and mind-expanding moments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93289202-5297-4059-9566-0b6aacc11209",
   "metadata": {},
   "source": [
    "## 03 - Examples of linear algebra in machine learning\n",
    "Linear algebra is concerned with vectors, matrices and linear transforms. It is foundational to machine learning from notations used to describe algorithms operation to their implementation in code. The relationship between linear algebra and machine learning is often left unexplained or abstract. Here are some examples of how linear algebra is leveraged in machine learning.\n",
    "\n",
    "### Dataset and data files\n",
    "Data is a matrix, which can be split into inputs (a matrix $X$) and outputs (a vector $y$. Each row has the same length (same number of columns): the data is vectorized and can be passed to a model one by one or in batch. The model can be pre-configured to expect rows of a fixed width.\n",
    "\n",
    "### Images and photographs\n",
    "An image is a table structure with a width and height and one-pixel value in each cell for black and white images or three pixel values (red, green and blue) for color images. Operations such as cropping, scaling, shearing are described using linear algebra notations and operations.\n",
    "\n",
    "### One hot encoding\n",
    "Categorical data can be one hot encoded so they are easier to work with and learn from by some machine learning techniques. One column is created for each category and a row for each example (e.g. if the categories are red, green and blue, we create a red column, a green column and a blue column). For each row in the dataset, we enter 1 in the column corresponding to the category and 0 in the others. Each row is encoded as a binary vector (0 or 1), which is an example of sparse representation.\n",
    "\n",
    "### Linear regression\n",
    "Linear regression is used to describe the relationship between variables. Solving the linear regression problem means finding a set of coefficients that gives the best prediction of the output variable when multiplied by each of the input variable and added together. It is usually solved using least squares optimization leveraging matrix factorization such as LU decomposition or singular-value decomposition.\n",
    "\n",
    "It can be summarized using linear algebra notation:  \n",
    "$y = A \\cdot b$\n",
    "where\n",
    "- $y$ is the output variable\n",
    "- $A$ is the dataset\n",
    "- $b$ are the model coefficients\n",
    "\n",
    "### Regularization\n",
    "Simpler models often have smaller coefficient values. Regularization is leveraged to encourage a model to minimize the size of coefficients. Common implementations are the $L^{1}$ and $L^{2}$ forms. Both are a measure of the length of the coefficients as a vector, and leverage the vector norm.\n",
    "\n",
    "### Principal component analysis\n",
    "Modeling data with many features is challenging. Principal component analysis is a dimensionality reduction method used to create projections of high-dimensional data for visualization and training models. It uses a matrix factorization method; more robust implementations leverage eigendecomposition and singular-value decomposition.\n",
    "\n",
    "### Singular-value decomposition\n",
    "Singular-value decomposition is a dimensionality reduction method with applications in feature selection, visualization and noise reduction.\n",
    "\n",
    "### Latent semantic analysis\n",
    "Latent semantic analysis, also called latent semantic indexing, is a natural language processing method applied to document-term matrices (sparse representations of a text) and distill the representation down to its most relevant essence using matrix factorization methods such as singular-value decomposition.\n",
    "\n",
    "### Recommender systems\n",
    "The similarity between sparse customer behavior vectors leverages distance measures (e.g. Euclidean distance) or dot products. Matrix factorization methods such as single-value decomposition are used to distill user data to their essence for querying, searching and comparison.\n",
    "\n",
    "### Deep learning\n",
    "Artificial neural networks are nonlinear machine learning algorithms inspired by the way our brain processes information and have proved effective at a range of problems such as machine translation, photo captioning or speech recognition. Their execution leverages linear algebra structures (vectors, matrices and tensors of inputs and coefficients) multiplied and added together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284b3ea-e39d-4b13-9448-beaee007e579",
   "metadata": {},
   "source": [
    "## 04 - Introduction to NumPy arrays\n",
    "### NumPy n-dimensional array\n",
    "NumPy is the preferred Python tool for linear algebra operations:\n",
    "- the main structure is the `ndarray`, short for n-dimensional array\n",
    "- data in an `ndarray` is referred to as an array\n",
    "- data in an `ndarray` must be of the same type\n",
    "- the type of an `ndarray` can be retrieved using the argument `.dtype` on the array\n",
    "- the shape (ength of each dimension) of an `ndarray` can be retrieved using the argument `.shape` on the array\n",
    "- the function `array()` is used to create an `ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbb45041-6caa-4193-b85a-586bb7e22c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array_int = [1 2 3]\n",
      "array_float = [1.09  2.87  3.654]\n",
      "array_mixed = [1.  2.5 3. ]\n",
      "\n",
      "Type of array_int: int64\n",
      "Type of array_float: float64\n",
      "Type of array_mixed: float64\n",
      "==> <array_mixed> was passed an array of mixed data types (integers and floats)\n",
      "and NumPy forced all the `ndarray` to a float dtype)\n",
      "\n",
      "Shape of array_int: (3,)\n",
      "Shape of array_float: (3,)\n",
      "Shape of array_mixed: (3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create arrays of integer, float and mixed types\n",
    "array_int = np.array([1, 2, 3])\n",
    "array_float = np.array([1.09, 2.87, 3.654])\n",
    "array_mixed = np.array([1, 2.5, 3])\n",
    "\n",
    "# Print arrays\n",
    "print(f\"array_int = {array_int}\")\n",
    "print(f\"array_float = {array_float}\")\n",
    "print(f\"array_mixed = {array_mixed}\")\n",
    "    \n",
    "# Get the shape of all arrays\n",
    "print(f\"\\nType of array_int: {array_int.dtype}\")\n",
    "print(f\"Type of array_float: {array_float.dtype}\")\n",
    "print(f\"\"\"Type of array_mixed: {array_mixed.dtype}\n",
    "==> <array_mixed> was passed an array of mixed data types (integers and floats)\n",
    "and NumPy forced all the `ndarray` to a float dtype)\"\"\"\n",
    "\n",
    "# Get the type of all arrays\n",
    "print(f\"\\nShape of array_int: {array_int.shape}\")\n",
    "print(f\"Shape of array_float: {array_float.shape}\")\n",
    "print(f\"Shape of array_mixed: {array_mixed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9728057-70f4-47b0-b21d-fa7fabb5ed8f",
   "metadata": {},
   "source": [
    "### Functions to create arrays\n",
    "- `empty()` creates an array of random variables of the specified shape\n",
    "- `zeros()` creates an array of zeros of the specified shape\n",
    "- `ones()` creates an array of ones variables of the specified shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e75fd652-1eab-4aa0-8946-1d72fc22574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array_empty =\n",
      "[[9.27119196e-310 0.00000000e+000 1.39067116e-309]\n",
      " [5.93760961e-038 2.14330648e+184 5.44909756e-090]\n",
      " [5.26237620e-037 1.38524351e-309 4.74303020e-322]]\n",
      "\n",
      "array_zeros =\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "\n",
      "array_ones =\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Create arrays\n",
    "array_empty = np.empty([3,3])\n",
    "array_zeros = np.zeros([3,5])\n",
    "array_ones = np.ones([3,5])\n",
    "\n",
    "# Print arrays\n",
    "print(f\"array_empty =\\n{array_empty}\")\n",
    "print(f\"\\narray_zeros =\\n{array_zeros}\")\n",
    "print(f\"\\narray_ones =\\n{array_ones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca86bd-6fa9-4da8-a108-9a69b4948c15",
   "metadata": {},
   "source": [
    "### Combining arrays\n",
    "Arrays can be stacked:\n",
    "- vertically using `vstack()`: given two one-dimensional arrays of the same length, you get a new two-dimensional array with two rows\n",
    "- horizontally using `hstack()`: given two one-dimensional arrays of potentially similar length, you get a new one-dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f931081a-eb3c-40bf-88d4-d915d61245d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same length:\n",
      "Vertical stack\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "Horizontal stack\n",
      "[1 2 3 4 5 6]\n",
      "\n",
      "Different length:\n",
      "Vertical stack\n",
      "ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 4\n",
      "\n",
      "Horizontal stack\n",
      "[1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "# Same length\n",
    "print(\"Same length:\")\n",
    "# Creating arrays\n",
    "array_s01 = np.array([1, 2, 3])\n",
    "array_s02 = np.array([4, 5, 6])\n",
    "\n",
    "# Stacking\n",
    "stack_sv = np.vstack([array_s01, array_s02])\n",
    "stack_sh = np.hstack([array_s01, array_s02])\n",
    "\n",
    "# Printing results\n",
    "print(f\"Vertical stack\\n{stack_sv}\")\n",
    "print(f\"\\nHorizontal stack\\n{stack_sh}\")\n",
    "\n",
    "# Different length\n",
    "print(\"\\nDifferent length:\")\n",
    "# Creating arrays\n",
    "array_d01 = np.array([1, 2, 3])\n",
    "array_d02 = np.array([4, 5, 6, 7])\n",
    "\n",
    "# Stacking\n",
    "print(f\"Vertical stack\")\n",
    "try:\n",
    "    stack_dv = np.vstack([array_d01, array_d02])\n",
    "    print(stack_dv)\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "stack_dh = np.hstack([array_d01, array_d02])\n",
    "\n",
    "# Printing results\n",
    "print(f\"\\nHorizontal stack\\n{stack_dh}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
